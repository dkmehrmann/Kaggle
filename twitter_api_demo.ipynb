{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import datetime\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Authorization\n",
    "It is good practice to store credentials in a text file so that if/when you push your notebook to Github, you don't have to remember to remove the credentials every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing...\n",
      "Authorization successful\n"
     ]
    }
   ],
   "source": [
    "print(\"Authorizing...\")\n",
    "\n",
    "with open('twitter_auth.txt') as f:\n",
    "    file_content = f.readlines()\n",
    "    file_content = [x.strip() for x in file_content]\n",
    "\n",
    "CONSUMER_KEY = file_content[0]\n",
    "CONSUMER_SECRET = file_content[1]\n",
    "OAUTH_TOKEN = file_content[2]\n",
    "OAUTH_TOKEN_SECRET = file_content[3]\n",
    "\n",
    "#twitter authorization\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.TwitterStream(auth=auth)\n",
    "  \n",
    "if (not twitter_api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "print(\"Authorization successful\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming API\n",
    "The streaming API rate limit is not publicized. For this API, you are not limited by number of tweets, but number of requests. A request is like a unique opening of the Twitter Stream. If you need to make multiple requests, it is possible to lump them all together into one request. If you need to keep track of which request corresponds to which tweet, however, you will have to loop through them individually. This is when you might run into rate limiting problems because each query constitutes a separate request. \n",
    "\n",
    "For example, if you want tweets that you think might be questions, you would run a single query (request) for anything that has a character in `['?', 'where', 'what', 'how',...]` as you wouldn't need to keep track of which tweet came from which query. \n",
    "\n",
    "If you wanted 200 tweets from each of `['?', 'where', 'what', 'how']` however, you might need to make separate requests for each. Additionally, there is some limit on how many queries you can jam into one request. \n",
    "\n",
    "### From [Twitter](https://dev.twitter.com/streaming/overview/connecting):\n",
    ">Rate limiting\n",
    ">Clients which do not implement backoff and attempt to reconnect as often as possible will have their connections rate limited for a small number of minutes. Rate limited clients will receive HTTP 420 responses for all connection requests.\n",
    "\n",
    ">Clients which break a connection and then reconnect frequently (to change query parameters, for example) run the risk of being rate limited.\n",
    "\n",
    ">Twitter does not make public the number of connection attempts which will cause a rate limiting to occur, but there is some tolerance for testing and development. A few dozen connection attempts from time to time will not trigger a limit. However, it is essential to stop further connection attempts for a few minutes if a HTTP 420 response is received. If your client is rate limited frequently, it is possible that your IP will be blocked from accessing Twitter for an indeterminate period of time.\n",
    "\n",
    "Also:\n",
    "\n",
    ">Back off exponentially for HTTP 420 errors. Start with a 1 minute wait and double each attempt. Note that every HTTP 420 received increases the time you must wait until rate limiting will no longer will be in effect for your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the queries we are going to run\n",
    "qs = ['the', 'an', 'it', 'who', 'were']\n",
    "\n",
    "# the twitter stream object\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "\n",
    "# initialize the counters\n",
    "requests = 0\n",
    "backoff_timer = 60 # this is how long we'll sleep if we get rate limited\n",
    "sleep_timer = 0 # this is how long we'll sleep after each query\n",
    "\n",
    "uids = []\n",
    "\n",
    "# we are going to iterate a few times to demonstrate\n",
    "for iters in range(0,3):\n",
    "    \n",
    "    # for each query in the queries\n",
    "    for q in qs:\n",
    "        \n",
    "        requests += 1 # count of requests made\n",
    "        count=0 # count of tweets per query\n",
    "        \n",
    "        ### this is the chunk that handles rate limiting ################\n",
    "        \n",
    "        while True:\n",
    "            try: # try to open the stream\n",
    "                stream = twitter_stream.statuses.filter(track=q)\n",
    "            \n",
    "            except Exception as e: # if it doesn't work (i.e. we were limited)\n",
    "                print('rate limited...sleeping for {0} seconds'.format(sleep_timer))\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(backoff_timer) # 'back off' for a certain amount of time\n",
    "                backoff_timer = backoff_timer * 2 # double the backoff timer\n",
    "                \n",
    "                ### since we got rate limited, we must not be sleeping long enough per request\n",
    "                sleep_timer = sleep_timer + 2 # add 2 seconds to the sleep timer\n",
    "                \n",
    "            break\n",
    "            \n",
    "        ###################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        for tweet in stream:\n",
    "\n",
    "                uids.appned(tweet['user']['id'])\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "                if count % 100 == 0:\n",
    "                    \n",
    "                    print('Request {0} complete'.format(requests))\n",
    "                    print('100 tweets seen from \"{0}\"'.format(q))\n",
    "                    print(datetime.datetime.now())\n",
    "                    \n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    break\n",
    "        time.sleep(sleep_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = twitter.Twitter(auth=auth)\n",
    "\n",
    "tweets = t.statuses.user_timeline(screen_name=\"billybob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239257191"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
