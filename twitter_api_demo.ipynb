{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import datetime\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Authorization\n",
    "It is good practice to store credentials in a text file so that if/when you push your notebook to Github, you don't have to remember to remove the credentials every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing...\n",
      "Authorization successful\n"
     ]
    }
   ],
   "source": [
    "print(\"Authorizing...\")\n",
    "\n",
    "with open('twitter_auth.txt') as f:\n",
    "    file_content = f.readlines()\n",
    "    file_content = [x.strip() for x in file_content]\n",
    "\n",
    "CONSUMER_KEY = file_content[0]\n",
    "CONSUMER_SECRET = file_content[1]\n",
    "OAUTH_TOKEN = file_content[2]\n",
    "OAUTH_TOKEN_SECRET = file_content[3]\n",
    "\n",
    "#twitter authorization\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.TwitterStream(auth=auth)\n",
    "  \n",
    "if (not twitter_api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "print(\"Authorization successful\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming API\n",
    "The streaming API rate limit is not publicized. For this API, you are not limited by number of tweets, but number of requests. A request is like a unique opening of the Twitter Stream. If you need to make multiple requests, it is possible to lump them all together into one request. If you need to keep track of which request corresponds to which tweet, however, you will have to loop through them individually. This is when you might run into rate limiting problems because each query constitutes a separate request. \n",
    "\n",
    "For example, if you want tweets that you think might be questions, you would run a single query (request) for anything that has a character in `['?', 'where', 'what', 'how',...]` as you wouldn't need to keep track of which tweet came from which query. \n",
    "\n",
    "If you wanted 200 tweets from each of `['?', 'where', 'what', 'how']` however, you might need to make separate requests for each. Additionally, there is some limit on how many queries you can jam into one request. \n",
    "\n",
    "### From [Twitter](https://dev.twitter.com/streaming/overview/connecting):\n",
    ">Rate limiting\n",
    ">Clients which do not implement backoff and attempt to reconnect as often as possible will have their connections rate limited for a small number of minutes. Rate limited clients will receive HTTP 420 responses for all connection requests.\n",
    "\n",
    ">Clients which break a connection and then reconnect frequently (to change query parameters, for example) run the risk of being rate limited.\n",
    "\n",
    ">Twitter does not make public the number of connection attempts which will cause a rate limiting to occur, but there is some tolerance for testing and development. A few dozen connection attempts from time to time will not trigger a limit. However, it is essential to stop further connection attempts for a few minutes if a HTTP 420 response is received. If your client is rate limited frequently, it is possible that your IP will be blocked from accessing Twitter for an indeterminate period of time.\n",
    "\n",
    "Also:\n",
    "\n",
    ">Back off exponentially for HTTP 420 errors. Start with a 1 minute wait and double each attempt. Note that every HTTP 420 received increases the time you must wait until rate limiting will no longer will be in effect for your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1 complete\n",
      "100 tweets seen from \"the\"\n",
      "2015-11-09 17:18:25.279737\n",
      "Request 2 complete\n",
      "100 tweets seen from \"an\"\n",
      "2015-11-09 17:18:29.610144\n",
      "Request 3 complete\n",
      "100 tweets seen from \"it\"\n",
      "2015-11-09 17:18:37.037874\n",
      "Request 4 complete\n",
      "100 tweets seen from \"who\"\n",
      "2015-11-09 17:18:47.391463\n",
      "Request 5 complete\n",
      "100 tweets seen from \"were\"\n",
      "2015-11-09 17:18:56.465825\n",
      "Request 6 complete\n",
      "100 tweets seen from \"the\"\n",
      "2015-11-09 17:19:06.781647\n",
      "\n",
      "rate limited...sleeping for 60 seconds\n",
      "\n",
      "Request 7 complete\n",
      "100 tweets seen from \"an\"\n",
      "2015-11-09 17:20:13.256494\n",
      "Request 8 complete\n",
      "100 tweets seen from \"it\"\n",
      "2015-11-09 17:20:20.055611\n",
      "Request 9 complete\n",
      "100 tweets seen from \"who\"\n",
      "2015-11-09 17:20:26.719127\n",
      "Request 10 complete\n",
      "100 tweets seen from \"were\"\n",
      "2015-11-09 17:20:38.724599\n",
      "Request 11 complete\n",
      "100 tweets seen from \"the\"\n",
      "2015-11-09 17:20:45.472743\n",
      "Request 12 complete\n",
      "100 tweets seen from \"an\"\n",
      "2015-11-09 17:20:53.342037\n",
      "Request 13 complete\n",
      "100 tweets seen from \"it\"\n",
      "2015-11-09 17:20:58.000863\n",
      "\n",
      "rate limited...sleeping for 120 seconds\n",
      "\n",
      "Request 14 complete\n",
      "100 tweets seen from \"who\"\n",
      "2015-11-09 17:23:13.430538\n",
      "Request 15 complete\n",
      "100 tweets seen from \"were\"\n",
      "2015-11-09 17:23:36.666349\n"
     ]
    }
   ],
   "source": [
    "# the queries we are going to run\n",
    "qs = ['the', 'an', 'it', 'who', 'were']\n",
    "\n",
    "# the twitter stream object\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "\n",
    "# initialize the counters\n",
    "requests = 0\n",
    "backoff_timer = 60 # this is how long we'll sleep if we get rate limited\n",
    "sleep_timer = 0 # this is how long we'll sleep after each query\n",
    "uids = []\n",
    "\n",
    "# we are going to iterate a few times to demonstrate\n",
    "for iters in range(0,3):\n",
    "    \n",
    "    # for each query in the queries\n",
    "    for q in qs:\n",
    "        \n",
    "        requests += 1 # count of requests made\n",
    "        count=0 # count of tweets per query\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### this is the chunk that handles rate limiting ################\n",
    "        \n",
    "        while True:\n",
    "            try: # try to open the stream\n",
    "                stream = twitter_stream.statuses.filter(track=q)\n",
    "            \n",
    "            except Exception as e: # if it doesn't work (i.e. we were limited)\n",
    "                print('\\nrate limited...sleeping for {0} seconds\\n'.format(backoff_timer))\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(backoff_timer) # 'back off' for a certain amount of time\n",
    "                backoff_timer = backoff_timer * 2 # double the backoff timer\n",
    "                \n",
    "                ### since we got rate limited, we must not be sleeping long enough per request\n",
    "                sleep_timer = sleep_timer + 2 # add 2 seconds to the sleep timer\n",
    "                \n",
    "            break\n",
    "            \n",
    "        ###################################################################\n",
    "        \n",
    "        \n",
    "        ### get user id's from our stream #################################\n",
    "        \n",
    "        for tweet in stream:\n",
    "                try:\n",
    "                    uids.append(tweet['user']['id'])\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "                if count % 100 == 0: # for every hundred tweets we get\n",
    "                    \n",
    "                    print('Request {0} complete'.format(requests))\n",
    "                    print('100 tweets seen from \"{0}\"'.format(q))\n",
    "                    print(datetime.datetime.now())\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    break # go to the next query\n",
    "        ###################################################################\n",
    "        \n",
    "        \n",
    "        time.sleep(sleep_timer) # this is the key to not getting rate limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###REST API\n",
    "\n",
    "For user timelines and other requests from the REST API, the [rate limits are made public](https://dev.twitter.com/rest/public/rate-limits). These rate limits are defined as a maximum number of requests per 15 minute window. For pulling user timelines, the maximum is 180 requests per 15 minutes. There is also a maximum number of tweets you can get per request.\n",
    "\n",
    "From [Twitter](https://dev.twitter.com/rest/reference/get/statuses/user_timeline) on the `count` parameter:\n",
    ">Specifies the number of tweets to try and retrieve, **up to a maximum of 200 per distinct request**. The value of count is best thought of as a limit to the number of tweets to return because suspended or deleted content is removed after the count has been applied. We include retweets in the count, even if include_rts is not supplied. It is recommended you always send include_rts=1 when using this API method.\n",
    "\n",
    "If more than 200 tweets are desired, pagination must me used. See [Working with Timelines](https://dev.twitter.com/rest/public/timelines).\n",
    "\n",
    "Since Twitter may not honor our request from time to time, it is helpful to find out why. This code will handle rate limiting in a special way and print out all other errors. For help with the error codes see [here](https://dev.twitter.com/overview/api/response-codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 requests made, sleeping for 796 seconds\n",
      "Failed on 48218817 with error 401\n"
     ]
    }
   ],
   "source": [
    "# get the twitter object\n",
    "t = twitter.Twitter(auth=auth)\n",
    "\n",
    "# initialize the counter\n",
    "counter = 0\n",
    "\n",
    "#initialize the start time\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# for each user (just the first 300)\n",
    "for user in uids[:300]:\n",
    "    counter += 1\n",
    "    \n",
    "    while True:\n",
    "        try: # try to get a user's timeline\n",
    "            tweets = t.statuses.user_timeline(user_id=user, count=100)\n",
    "        except Exception as e: # if it doesn't work\n",
    "            if e.e.code == 429: # the error code for rate limit with REST API is 429\n",
    "                print('Rate Limited! sleeping for 1 minute')\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(60)\n",
    "            else: # if not rate limit, print the error and user\n",
    "                print('Failed on {0} with error {1}'.format(user,e.e.code))\n",
    "                break\n",
    "        break\n",
    "    \n",
    "    # for every 170 requests (to stay below limit)\n",
    "    if counter % 170 == 0:\n",
    "        # get the elapsed time\n",
    "        elapsed = datetime.datetime.now() - start\n",
    "        # calculate how long to sleep (if it is negative, set to 0)\n",
    "        # right now we make 170 requests every 16 minute window to stay below limit\n",
    "        sleep_timer = max((16*60 - elapsed.seconds),0) \n",
    "        print('{0} requests made, sleeping for {1} seconds'.format(counter, sleep_timer))\n",
    "        sys.stdout.flush()\n",
    "        # sleep for exactly how long we need to\n",
    "        time.sleep(sleep_timer)\n",
    "        # reset the timer\n",
    "        start = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 401 error means 'Not Authorized' which means that user has a private account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
